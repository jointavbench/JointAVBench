# JointAVBench: A Benchmark for Joint Audio-Visual Reasoning Evaluation

<div align="center">

[![Paper](https://img.shields.io/badge/Paper-PDF-red)](https://arxiv.org/pdf/2512.12772)
[![Project Page](https://img.shields.io/badge/Project-Page-purple)](https://jointavbench.github.io)
[![Hugging Face](https://img.shields.io/badge/ğŸ¤—-Benchmark-yellow)](https://huggingface.co/datasets/JointAVBench/JointAVBench)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)

</div>

---

## ğŸ“– Overview

**JointAVBench** is a comprehensive benchmark for evaluating omni-modal large language models (LLMs) on joint audio-visual reasoning tasks. This repository showcases our benchmark generation pipeline and provides evaluation tools for assessing models on questions that require both visual and auditory information to answer correctly.

### âœ¨ Key Features

- **Large-scale Benchmark**: 2,853 questions across 15 diverse task types
- **Automated Generation Pipeline**: State-of-the-art vision-LLMs, audio-LLMs, and LLMs automatically synthesize questions requiring joint audio-visual reasoning
- **Multi-dimensional Coverage**:
  - **5 cognitive dimensions**: Temporal, Spatial, Long-form, Emotional, and Plot understanding
  - **4 audio information types**: Speech, Sound events, Music, and Speech emotion
  - **3 scene spans**: Single-scene, Multi-scene, and Full-scene reasoning
- **Comprehensive Evaluation**: Evaluation suite for majority mainstream omni-modal models
- **Challenging Tasks**: Multi-scene tasks requiring complex cross-modal reasoning

### ğŸ“„ Abstract

Compared to vision or audio large language models (LLMs), the key advantage of omni large language model lies in their joint audio-visual reasoning capability. To train such models, datasets with questions requiring both visual and auditory information to answer are needed. Moreover, videos contain complex audio signal types and scenes, interleaved with each other, demanding models with various cognitive capabilities. However, current datasets lack challenging multi-scene tasks, various types of audio information and cognition abilities.

This paper introduces **JointAVBench**, a dataset designed to answer questions that necessitate AV integration, spanning 5 cognitive dimensions, 4 audio information types, and 3 scene spans. Our benchmark reveals that the top omni-LLM achieves only 56.2% average accuracy, highlighting significant room for improvement, particularly in cross-scene reasoning.

## ğŸ“‘ Table of Contents

- [Benchmark Overview](#-benchmark-overview)
- [Generation Pipeline](#-generation-pipeline)
- [Dataset](#-dataset)
- [Evaluation](#-evaluation)
- [Results](#-results)
- [Citation](#-citation)
- [License](#-license)

## ğŸ¯ Benchmark Overview

### ğŸ“Š Task Categories

JointAVBench consists of **2,853 questions** across **15 distinct tasks** spanning multiple dimensions. Details about each task category can be found in the [paper](https://arxiv.org/pdf/2512.12772).

### ğŸ¨ Example Questions

<div align="center">
  <img src="images/examples.jpg" alt="Example questions from JointAVBench" width="90%">
  <p><em>Sample questions from different task categories showcasing the diversity of our benchmark</em></p>
</div>

### ğŸ§  Cognitive Dimensions

- **Temporal (tem)**: Understanding time-based relationships and ordering
- **Spatial (spa)**: Localizing sounds and understanding spatial relationships
- **Long-form (long)**: Processing and reasoning over extended content
- **Emotional (emo)**: Recognizing and understanding emotional content
- **Plot (plo)**: Comprehending narratives and storylines

## ğŸ”„ Generation Pipeline

Our automated benchmark generation pipeline consists of several stages:

<div align="center">
  <img src="images/framework.png" alt="JointAVBench generation framework" width="90%">
  <p><em>Complete pipeline for automated benchmark generation from raw videos</em></p>
</div>


## ğŸ“¦ Dataset

### â¬‡ï¸ Download the Benchmark

The JointAVBench dataset is available on Hugging Face:

```bash
# Download benchmark questions and videos
pip install huggingface_hub
huggingface-cli download JointAVBench/JointAVBench --local-dir ./data
```

The benchmark file `jointavbench.json` contains all 2,853 questions with metadata. Please note that due to content restrictions, we cannot share the raw videos. However, we provide a url to the original YouTube video for each question.

### ğŸ“‚ Dataset Structure

```
JointAVBench/
â”œâ”€â”€ annotations/                # Annotation data
â”œâ”€â”€ jointavbench.json           # Evaluation code
â”œâ”€â”€ README.md
```

### ğŸ“‹ Data Format

Each question in the benchmark follows this format:

```json
{
  "qid": "-CEDoGn0w1s_task1_0",
  "video_name": "-CEDoGn0w1s",
  "task": "STL",
  "question": "Which objects are mentioned only in the dialogue but not clearly shown in the video, and when does the first object appear in the dialogue?",
  "correct_answer": "The broom, mentioned at around 6.34s",
  "explanation": "The object \"broom\" is mentioned in the dialogue but does not appear in the video description. It is the first object mentioned in the dialogue, appearing at around 6.34s.",
  "options": [
    "The shovel, mentioned at around 6.34s",
    "The keys, mentioned at around 3.36s",
    "The hat, mentioned at around 12.76s",
    "The broom, mentioned at around 6.34s"
  ],
  "video_url": "https://www.youtube.com/watch?v=-CEDoGn0w1s",
  "segment_timestamp": [653.444, 699.657]
}
```

## ğŸ”¬ Evaluation

### ğŸš€ Quick Start

For detailed evaluation instructions, see **[evaluation/README.md](evaluation/README.md)**.

```bash
# Navigate to evaluation folder
cd evaluation

# Set up environment (see evaluation/README.md)
cp .env.example .env
# Edit .env with your API keys and model paths

# Run evaluation
python evaluation.py --qa-path ../data/benchmark.json --model-name gemini --modality av
```

### ğŸ¤– Supported Models

We provide evaluation scripts for 10+ models:

- **API-based**: Gemini, GPT-4o, Kimi-Audio
- **Open-source**: LLaVA-Video, InternVL, Qwen-VL/Audio/Omni, Video-LLaMA 2/3, AVicuna

See [evaluation/README.md](evaluation/README.md) for complete setup and usage instructions.

## ğŸ“Š Results

Our benchmark reveals significant challenges for current omni-modal models:

- **Top Performance**: 56.2% average accuracy
- **Cross-scene Reasoning**: Particularly challenging (42-50% accuracy)
- **Single-scene Tasks**: Relatively better (68% accuracy)
- **Performance Gaps**: Significant variations across cognitive dimensions and audio types

<div align="center">
  <img src="./images/results.png" alt="JointAVBench Results" width="90%">
  <p><em>Performance comparison of different models across task categories</em></p>
</div>

### ğŸ” Key Findings

1. **Multi-modal Reasoning is Hard**: Even top models struggle to integrate audio-visual information effectively
2. **Scene Complexity Matters**: Performance degrades significantly for multi-scene and full-video tasks
3. **Audio Type Dependency**: Models perform differently on speech vs. music vs. sound events
4. **Cognitive Dimension Gaps**: Temporal and spatial reasoning show better results than plot understanding

For detailed results and analysis, please refer to our [paper](https://arxiv.org/pdf/2512.12772).

## ğŸ“ Project Structure

```
jointavbench/
â”œâ”€â”€ README.md                   # This file (project overview)
â”œâ”€â”€ LICENSE                     # Apache 2.0 License
â”œâ”€â”€ images/                    # Figures and visualizations
â”‚   â”œâ”€â”€ framework.png         # Pipeline architecture diagram
â”‚   â””â”€â”€ examples.png          # Example questions showcase
â”œâ”€â”€ evaluation/               # Evaluation code and documentation
â”‚   â”œâ”€â”€ README.md            # Complete evaluation guide
â”‚   â”œâ”€â”€ .env.example         # Environment configuration template
â”‚   â”œâ”€â”€ evaluation.py        # Main evaluation script
â”‚   â””â”€â”€ eval_*.py            # Model-specific evaluation modules
â”œâ”€â”€ gen_caption/             # Caption generation module
â”‚   â”œâ”€â”€ README.md           # Caption generation documentation
â”‚   â”œâ”€â”€ audio_caption.py    # Audio caption generation
â”‚   â”œâ”€â”€ video_caption.py    # Video caption generation
â”‚   â””â”€â”€ utils.py            # Caption utilities
â””â”€â”€ generation_pipeline/    # Benchmark generation pipeline
    â”œâ”€â”€ README.md          # Pipeline documentation
    â”œâ”€â”€ identify_intervals.py   # Interval identification
    â”œâ”€â”€ generate_qa.py          # QA generation
    â”œâ”€â”€ generate_distractor.py # Distractor generation
    â”œâ”€â”€ *_check.py             # Quality control scripts
    â””â”€â”€ utils.py               # Pipeline utilities
```

## ğŸ“ Citation

If you find JointAVBench useful for your research, please cite our paper:

```bibtex
@article{chao2025jointavbench,
  title={JointAVBench: A Benchmark for Joint Audio-Visual Reasoning Evaluation},
  author={Chao, Jianghan and Gao, Jianzhang and Tan, Wenhui and Sun, Yuchong and Song, Ruihua and Ru, Liyun},
  journal={arXiv preprint arXiv:2512.12772},
  year={2025}
}
```

## ğŸ“„ License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

We thank the following projects and resources:

- [Qwen](https://github.com/QwenLM/Qwen) for audio and vision models
- The research community for open-source multimodal models

## ğŸ“® Contact

For questions and feedback:

- **Evaluation Issues**: See [evaluation/README.md](evaluation/README.md)
- **Bug Reports**: Open an issue on GitHub
- **Project Page**: [https://jointavbench.github.io](https://jointavbench.github.io)
- **Email**: chaojh@ruc.edu.cn

---

<div align="center">

**For detailed evaluation instructions and environment setup, see [evaluation/README.md](evaluation/README.md)**

</div>
